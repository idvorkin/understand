# Is AI sentient and is it even useful to ask?

**June 2022.** Blake Lemoine, an engineer at Google, claims that their new AI
is sentient [and is fired](https://www.theverge.com/2022/7/22/23274958/google-
ai-engineer-blake-lemoine-chatbot-lamda-2-sentience) _(The Verge)._

_Although, not quite. You can piece what actually happened from Lemoine’s
own[contemporary Medium article](https://cajundiscordian.medium.com/may-be-
fired-soon-for-doing-ai-ethics-work-802d8c474e66) and the subsequent
[Washington Post
piece](https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-
blake-lemoine/) [[no paywall](https://archive.ph/20o90)]: Lemoine shared a doc
around Google titled “Is LaMDA Sentient?” (LaMDA is the name of the AI, a
large language model like GPT-3) – a colleague said this was "a bit
provocative." He started to speak with people outside the company and was
placed on disciplinary leave for violating confidentiality. Lemoine upped the
ante, "inviting a lawyer to represent LaMDA," and then you’re kinda done I
reckon. But the point is that the question was asked._

Can an AI be sentient?

Are there already sentient AIs, and if not now then when? 1,000 years from
now? Surely. 100 years? Probably. So 10 years? Maybe. How about 2025?
Tomorrow?

How could we tell?

Would it matter?

I’m going to muddle sentience and consciousness here because I don’t want to
get lost in definitions.

Wikipedia’s article on [Sentience](https://en.wikipedia.org/wiki/Sentience)
cites philosopher Antonio Damasio and says that "sentience is a minimalistic
way of defining consciousness" and limits it to "the capacity to feel
sensations and emotions."

According to this view: consciousness = sentience + creativity + intelligence

- sapience + self-awareness + intentionality + more.

I’d prefer to say that our terms are ill-defined, and that consciousnesses may
have all kinds of different characteristics, and may be a matter of degree.

So let’s enlarge the question, and agree to come back to pinning down terms
later: can an AI be conscious?

**2017.** Philosopher Susan Schneider proposes ACT: the AI Consciousness Test.

The idea is that consciousness is something that is felt: "we can all
experience what it feels like, from the inside, to exist."

So the question for ACT is "whether the synthetic minds we create have an
experience-based understanding of the way it feels, from the inside, to be
conscious."

i.e. do AIs feel the same as we do?

The proposed test is a series of questions.

Thus, the ACT would challenge an AI with a series of increasingly demanding
natural language interactions to see how _quickly_ and _readily_ it can grasp
and use concepts and scenarios based on the internal experiences we associate
with consciousness. At the most elementary level we might simply ask the
machine if it conceives of itself as anything other than its physical self. At
a more advanced level, we might see how it deals with ideas and scenarios such
as those mentioned in the previous paragraph. At an advanced level, its
ability to reason about and discuss philosophical questions such as “the hard
problem of consciousness” would be evaluated. At the most demanding level, we
might see if the machine invents and uses such a consciousness-based concept
on its own, without relying on human ideas and inputs.

_(Article by Susan Schneider and Edwin Turner.)_

One problem - as with GPT-3/ChatGPT - is that large language models are
extraordinary mimics. So maybe they just _say_ the right stuff to pass the
test.

Schneider’s suggestion is to “box in” the AI away from human culture until
we’ve tested it against the ACT, so it can’t make guesses.

I don’t know. I’m more convinced by the _“quickly and readily”_ component of
ACT. Surely there are some puzzles that are quicker to deduce if you have
self-awareness? Dunno.

The AI Consciousness Test is one in a long line of tests for machine
intelligence, such as the Turing Test.

**2020.** There’s a solid critique of ACT in this paper by David Udell and
Eric Schwitzgebel, [Susan Schneider’s Proposed Tests for AI Consciousness:
Promising but
Flawed](http://faculty.ucr.edu/~eschwitz/SchwitzAbs/SchneiderCrit.htm) (PDF at
that link).

The challenge is that there’s always going to be a lower-level explanation of
how the AI is answering questions on the silicon substrate (a giant lookup
table, matrix maths, whatever), and that no series of questions is going to be
sufficient to convince people that there is genuine machine consciousness at a
higher level too.

One for the philosophers.

But Udell & Schitzgebel are articulate on the _urgency_ of finessing ACT or
something ACT-like:

AI consciousness, despite its present science-fictional air, may soon become
an urgent practical issue. Within the next few decades, engineers might
develop AI systems that some people, rightly or wrongly, claim have conscious
experiences like ours. _We will then face the question of whether such AI
systems would deserve moral consideration akin to that we give to people._
There is already an emerging ‘robot rights’ movement which would surely be
energized by plausible claims of robot consciousness (Schwitzgebel and Garza
2015; Gunkel 2018; Ziesche and Yampolskiy 2019). So we need to think seriously
in advance about how to test for consciousness among apparently conscious
machines …

Schneider, in her _Scientific American_ piece above, broadens the urgency to
brain implants:

machine consciousness could impact the viability of brain-implant
technologies, like those to be developed by Elon Musk’s new company,
Neuralink. If AI cannot be conscious, then the parts of the brain responsible
for consciousness could not be replaced with chips without causing a loss of
consciousness. And, in a similar vein, a person couldn’t upload their brain to
a computer to avoid death because that upload wouldn’t be a conscious being.

Consciousness is hard hey.

Consciousness is _weird._

Let’s say that we agree that a silicon substrate can host consciousness.

Or that a group of organic cells, properly arranged etc, can host
consciousness.

There is a slippery slope…

Eric Schwitzgebel again:

"_The United States is literally, like you, phenomenally conscious._ That is,
the United States literally possesses a stream of experiences over and above
the experiences of its members considered individually."

If you’re a materialist, you probably think that rabbits have conscious
experiences. And you ought to think that. After all, rabbits are a lot like
us, biologically and neurophysiologically.

If you’re a materialist, you probably also think that conscious experience
would be present in a wide range of naturally evolved alien beings
behaviorally very similar to us even if they are physiologically very
different. And you ought to think that. After all, it would be insupportable
Earthly chauvinism to deny consciousness to alien species behaviorally very
similar to us, even if they are physiologically different.

But, I will argue, a materialist who accepts consciousness in hypothetical
weirdly formed aliens ought also to accept consciousness in spatially
distributed group entities. If you then also accept rabbit consciousness, you
ought also accept the possibility of consciousness in rather dumb group
entities.

Finally, the United States is a rather dumb group entity of the relevant sort
(or maybe even it’s rather smart, but that’s more than I need for my
argument).

If we set aside our prejudices against spatially distributed group entities,
we can see that the United States has all the types of properties that
materialists normally regard as indicative of consciousness.

_(I’ve added paragraph breaks.)_

Schwitzgebel asks us to take the perspective of a consciousness entity which
is much larger than us humans:

A planet-sized alien who squints might see the United States as a single,
diffuse entity consuming bananas and automobiles, wiring up communication
systems, touching the Moon, and regulating its smoggy exhalations – an entity
that can be evaluated for the presence or absence of consciousness.

…and the rest of the chapter goes on to show convincingly that, yes, even if
the USA isn’t conscious, it’s worthy of being _evaluated._

_(Do we need Schneider to write the USACT?)_

This is perilously close to
[panpsychism](https://en.wikipedia.org/wiki/Panpsychism), "the view that the
mind or a mindlike aspect is a fundamental and ubiquitous feature of reality."

We are conscious. My cat is conscious, although differently. Asteroids is
conscious; AI is conscious, why not. Mud is conscious; a stellar nebula has
its own nebula-like conscious. _(Olaf Stapledon, in Star Maker, way back in
1937, wrote beautifully and poignantly about the[culture of gas cloud
megatheria](/notes/2006/02/scifi/?p=22) at the dawn of the cosmos.)_

What’s the alternative?

Maybe silicon _can’t_ be conscious.

Maybe GPT-4, GPT-5, GPT-N, no matter how convincing, will be an AI
[p-zombie](https://en.wikipedia.org/wiki/Philosophical_zombie), "a
hypothetical being that is physically identical to and indistinguishable from
a normal person but does not have conscious experience, qualia, or sentience."

Which implies there’s a cut-off somewhere. And I’m not happy with that either
– I’m not ready to declare that my cat isn’t conscious, in her own cat way.

Everything is conscious.

Or nothing is conscious – except me. I’m not so sure about you.

Neither seems satisfying. Or useful?

Back to Eric Schwitzgebel, his paper (and forthcoming book) _The Weirdness of
the World,_ and the consciousness or otherwise of the USA…

Schwitzgebel asked philosopher Daniel Dennett, and he replied:

To the extent that the United States is radically unlike human beings, _it’s
unhelpful to ascribe consciousness to it._ Its behavior is impoverished
compared to ours and its functional architecture is radically unlike our own.
Ascribing consciousness to the United States is not as much straightforwardly
false is it is misleading. It invites the reader to too closely assimilate
human architecture and group architecture.

And I like this approach, in a general sense, because it acknowledges the
perspective from which we’re asking the question - being human - and therefore
implicitly accepts that there will be other perspectives which have different
answers.

The question is not: do we have conscious AIs?

It is more like: from our perspective, is there a non-misleading distinction
between non-conscious AI and hypothetical conscious AI, and do we have
conscious AIs in that sense?

AND THEN:

If an AI _were_ to pass an AI Consciousness Test, in the non-misleading sense
above, would it make any difference?

Udell & Schwitzgebel’s argument is that it’s meaningful in terms of robot
rights.

But chickens have chicken-consciousness and we industrialise their growth and
kill and eat them. Maybe the implication is that we ought to feel [more
gratitude when eating meat](/home/2019/06/06/grativore) \- if we eat meat at
all - and that it’s poisonous to us to ignore that.

Or maybe they _don’t_ have chicken-consciousness! Arguably we shouldn’t be
treating chickens like we do in any case. It’s hard to imagine that we would
treat them any worse even if we were certain they were lumps of 100%
unthinking rock.

The point is that it’s not a question we really engage with, as a society.
Maybe when it comes up with AI we collectively won’t care then, either.

So, for me, asking about AI consciousness is a way to winkle out these _other_
questions.

Yes it’s important that we know when, in 50 years or 5 years, the machines
wake up and we meet the first conscious AI. But if we then vary in our
treatment of that AI, we’ll then have to ask what’s different about chickens,
[talking dogs](/home/2023/01/04/interspecies), the Whanganui River in New
Zealand which was [granted legal
personhood](https://www.bbc.com/travel/article/20200319-the-new-zealand-river-
that-became-a-legal-person) _(BBC, 2017),_ the first uploaded nervous system -
the open source [OpenWorm virtual nemotode project](https://openworm.org) \-
the entire USA as a conscious entity, and well, each other.

Definitely useful questions to ask.
